{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_true,f_x):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = [max(i,epsilon)for i in f_x]\n",
    "    y_pred = [min(i,1-epsilon)for i in y_pred]\n",
    "    y_pred = np.array(y_pred)\n",
    "    loss = y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred)\n",
    "    return - np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(age, afford, ylabel,epoch, rate = 0.5):\n",
    "    w1 = w2 = 1\n",
    "    bias = 0\n",
    "    for i in range(epoch):\n",
    "        wsum = w1*age + w2*afford + bias\n",
    "        pred = sigmoid(wsum)\n",
    "        loss = logloss(ylabel, pred)\n",
    "\n",
    "        dj_dw1 = np.mean(np.dot(np.transpose(age),(pred- ylabel))*age)\n",
    "        dj_dw2 = np.mean(np.dot(np.transpose(age),(pred- ylabel))*afford)\n",
    "        dj_db = np.mean(np.dot(np.transpose(age),(pred- ylabel)))\n",
    "\n",
    "        w1 = w1 - (rate*dj_dw1)\n",
    "        w2 = w2 - (rate*dj_dw2)\n",
    "        bias = bias - (rate*dj_db) \n",
    "\n",
    "        \n",
    "\n",
    "        print(f\"w1:{w1}, w2:{w2}, bias:{bias}, loss = {loss}\")\n",
    "\n",
    "    return w1, w2, bias\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/prasun/GitDemo/Learning_Deep_Learning/Lesson4/data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['age','affordibility']],df.bought_insurance,test_size=0.2, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled['age'] = X_train_scaled['age'] / 100\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled['age'] = X_test_scaled['age'] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:0.7771797852151715, w2:0.6487068685824777, bias:-0.5520320636561066, loss = 0.7113403233723417\n",
      "w1:0.8874205043768422, w2:0.8225098041977243, bias:-0.2789131648321477, loss = 0.6156967320625555\n",
      "w1:0.8191426999625071, w2:0.714864617058007, bias:-0.448069887480275, loss = 0.6425210189389307\n",
      "w1:0.8588783442813638, w2:0.7775109031462946, bias:-0.34962572362725153, loss = 0.6207679440827207\n",
      "w1:0.8345980736418289, w2:0.739231197183064, bias:-0.40977954728375676, loss = 0.6314569483188796\n",
      "w1:0.8490802054403321, w2:0.7620633869554789, bias:-0.3739003919271046, loss = 0.6242587866516961\n",
      "w1:0.8403018553214588, w2:0.7482236457770751, bias:-0.3956485566360249, loss = 0.6283030886317081\n",
      "w1:0.8455746797073657, w2:0.7565366571962975, bias:-0.3825852529772468, loss = 0.6257630638566529\n",
      "w1:0.8423894039185179, w2:0.7515148259976637, bias:-0.39047670200367135, loss = 0.6272562095391566\n",
      "w1:0.8443071648957732, w2:0.7545383230338769, bias:-0.3857254923753363, loss = 0.6263424686361883\n",
      "w1:0.8431501680606198, w2:0.7527142289244009, bias:-0.3885919259759415, loss = 0.6268883196561691\n",
      "w1:0.8438473365023341, w2:0.7538133683595359, bias:-0.3868647068635863, loss = 0.6265574510713297\n",
      "w1:0.8434269335972105, w2:0.7531505709865934, bias:-0.387906245592496, loss = 0.6267562562467347\n",
      "w1:0.8436803296895984, w2:0.7535500693304481, bias:-0.38727846248072445, loss = 0.6266361684909979\n",
      "w1:0.8435275550890359, w2:0.7533092084737053, bias:-0.3876569581127488, loss = 0.6267084762340928\n",
      "w1:0.8436196492133932, w2:0.7534544019130074, bias:-0.3874287969938455, loss = 0.6266648541775081\n",
      "w1:0.8435641284786544, w2:0.7533668692231038, bias:-0.387566348363694, loss = 0.626691140150537\n",
      "w1:0.8435975982593293, w2:0.7534196368953392, bias:-0.3874834277358956, loss = 0.626675289558007\n",
      "w1:0.8435774208233978, w2:0.7533878256224742, bias:-0.38753341687896914, loss = 0.6266848435304361\n",
      "w1:0.8435895846375377, w2:0.753407002806929, bias:-0.38750328130339734, loss = 0.6266790833940127\n",
      "w1:0.8435822516796926, w2:0.7533954418373533, bias:-0.387521448541302, loss = 0.6266825556765876\n",
      "w1:0.8435866723203825, w2:0.7534024113159186, bias:-0.38751049650355657, loss = 0.6266804623478331\n",
      "w1:0.8435840073444847, w2:0.7533982097773408, bias:-0.38751709892132147, loss = 0.6266817242789557\n",
      "w1:0.8435856139162287, w2:0.753400742660721, bias:-0.3875131186760098, loss = 0.6266809635176204\n",
      "w1:0.8435846453982104, w2:0.7533992157178994, bias:-0.38751551815758656, loss = 0.6266814221370293\n",
      "w1:0.8435852292664315, w2:0.7534001362308606, bias:-0.38751407163721907, loss = 0.626681145658301\n",
      "w1:0.8435848772829839, w2:0.7533995813020018, bias:-0.3875149436682829, loss = 0.626681312332288\n",
      "w1:0.8435850894752159, w2:0.7533999158393044, bias:-0.3875144179668073, loss = 0.6266812118531621\n",
      "w1:0.8435849615556965, w2:0.7533997141643864, bias:-0.3875147348845356, loss = 0.6266812724266684\n",
      "w1:0.8435850386716262, w2:0.7533998357435547, bias:-0.38751454383155676, loss = 0.626681235910073\n",
      "w1:0.8435849921824967, w2:0.7533997624498823, bias:-0.38751465900732773, loss = 0.6266812579239948\n",
      "w1:0.843585020208341, w2:0.753399806634772, bias:-0.3875145895739296, loss = 0.6266812446529592\n",
      "w1:0.843585003313037, w2:0.7533997799980314, bias:-0.38751463143166476, loss = 0.6266812526533657\n",
      "w1:0.8435850134983249, w2:0.7533997960559178, bias:-0.3875146061978434, loss = 0.6266812478303421\n",
      "w1:0.8435850073581519, w2:0.7533997863754648, bias:-0.3875146214099839, loss = 0.6266812507378888\n",
      "w1:0.8435850110597384, w2:0.7533997922112994, bias:-0.3875146122393866, loss = 0.626681248985082\n",
      "w1:0.8435850088282472, w2:0.7533997886931826, bias:-0.3875146177678559, loss = 0.6266812500417568\n",
      "w1:0.8435850101734954, w2:0.7533997908140695, bias:-0.38751461443503354, loss = 0.6266812494047433\n",
      "w1:0.8435850093625162, w2:0.7533997895354987, bias:-0.3875146164442162, loss = 0.6266812497887652\n",
      "w1:0.8435850098514128, w2:0.7533997903062816, bias:-0.387514615232986, loss = 0.6266812495572587\n",
      "w1:0.8435850095566829, w2:0.7533997898416173, bias:-0.38751461596317277, loss = 0.6266812496968217\n",
      "w1:0.8435850097343601, w2:0.753399790121739, bias:-0.3875146155229816, loss = 0.6266812496126865\n",
      "w1:0.8435850096272478, w2:0.7533997899528683, bias:-0.3875146157883499, loss = 0.6266812496634071\n",
      "w1:0.8435850096918203, w2:0.7533997900546716, bias:-0.3875146156283732, loss = 0.6266812496328303\n",
      "w1:0.843585009652893, w2:0.7533997899932997, bias:-0.38751461572481477, loss = 0.6266812496512634\n",
      "w1:0.8435850096763602, w2:0.7533997900302976, bias:-0.38751461566667533, loss = 0.626681249640151\n",
      "w1:0.8435850096622131, w2:0.7533997900079935, bias:-0.3875146157017245, loss = 0.6266812496468502\n",
      "w1:0.8435850096707416, w2:0.7533997900214394, bias:-0.3875146156805952, loss = 0.6266812496428116\n",
      "w1:0.8435850096656001, w2:0.7533997900133336, bias:-0.3875146156933329, loss = 0.6266812496452462\n",
      "w1:0.8435850096686996, w2:0.7533997900182201, bias:-0.387514615685654, loss = 0.6266812496437786\n",
      "w1:0.8435850096668311, w2:0.7533997900152742, bias:-0.3875146156902832, loss = 0.6266812496446634\n",
      "w1:0.8435850096679576, w2:0.75339979001705, bias:-0.3875146156874926, loss = 0.62668124964413\n",
      "w1:0.8435850096672786, w2:0.7533997900159796, bias:-0.3875146156891749, loss = 0.6266812496444516\n",
      "w1:0.843585009667688, w2:0.753399790016625, bias:-0.3875146156881606, loss = 0.6266812496442578\n",
      "w1:0.8435850096674412, w2:0.7533997900162359, bias:-0.38751461568877216, loss = 0.6266812496443745\n",
      "w1:0.84358500966759, w2:0.7533997900164705, bias:-0.38751461568840356, loss = 0.626681249644304\n",
      "w1:0.8435850096675003, w2:0.7533997900163292, bias:-0.3875146156886257, loss = 0.6266812496443466\n",
      "w1:0.8435850096675543, w2:0.7533997900164143, bias:-0.3875146156884919, loss = 0.6266812496443209\n",
      "w1:0.8435850096675218, w2:0.753399790016363, bias:-0.3875146156885725, loss = 0.6266812496443364\n",
      "w1:0.8435850096675414, w2:0.7533997900163939, bias:-0.38751461568852397, loss = 0.6266812496443271\n",
      "w1:0.8435850096675296, w2:0.7533997900163754, bias:-0.3875146156885531, loss = 0.6266812496443328\n",
      "w1:0.8435850096675367, w2:0.7533997900163866, bias:-0.3875146156885355, loss = 0.6266812496443293\n",
      "w1:0.8435850096675324, w2:0.7533997900163798, bias:-0.38751461568854617, loss = 0.6266812496443313\n",
      "w1:0.8435850096675349, w2:0.7533997900163839, bias:-0.38751461568853973, loss = 0.6266812496443301\n",
      "w1:0.8435850096675332, w2:0.7533997900163814, bias:-0.3875146156885438, loss = 0.6266812496443308\n",
      "w1:0.8435850096675344, w2:0.753399790016383, bias:-0.38751461568854106, loss = 0.6266812496443305\n",
      "w1:0.8435850096675337, w2:0.7533997900163819, bias:-0.38751461568854273, loss = 0.6266812496443307\n",
      "w1:0.8435850096675341, w2:0.7533997900163826, bias:-0.3875146156885417, loss = 0.6266812496443305\n",
      "w1:0.8435850096675339, w2:0.7533997900163821, bias:-0.38751461568854234, loss = 0.6266812496443306\n",
      "w1:0.843585009667534, w2:0.7533997900163822, bias:-0.38751461568854206, loss = 0.6266812496443305\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443306\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n",
      "w1:0.843585009667534, w2:0.7533997900163821, bias:-0.3875146156885422, loss = 0.6266812496443307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.843585009667534, 0.7533997900163821, -0.3875146156885422)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(X_train_scaled['age'],X_train_scaled['affordibility'],y_train,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.w1 = 1\n",
    "        self.w2 = 1\n",
    "        self.b = 0\n",
    "\n",
    "    def fit(self,X,y,epochs):\n",
    "         self.w1,self.w2,self.b = self. gradient_descent(X[\"age\"],X[\"affordibility\"],y,epoch= epochs)\n",
    "\n",
    "\n",
    "    def gradient_descent(self, age, afford, ylabel,epoch, rate = 0.5):\n",
    "        w1 = w2 = 1\n",
    "        bias = 0\n",
    "        n = len(age)\n",
    "        for i in range(epoch):\n",
    "            wsum = w1*age + w2*afford + bias\n",
    "            pred = sigmoid(wsum)\n",
    "            loss = logloss(ylabel, pred)\n",
    "\n",
    "            dj_dw1 = (np.dot(np.transpose(age),(pred- ylabel)))/n\n",
    "            dj_dw2 = (np.dot(np.transpose(afford),(pred- ylabel)))/n\n",
    "            dj_db = np.mean((pred- ylabel))\n",
    "\n",
    "            w1 = w1 - (rate*dj_dw1)\n",
    "            w2 = w2 - (rate*dj_dw2)\n",
    "            bias = bias - (rate*dj_db) \n",
    "\n",
    "            \n",
    "\n",
    "            # print(f\"Iteration:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss = {loss}\")\n",
    "        return w1,w2,bias\n",
    "    \n",
    "    def predict(self,X):\n",
    "        f_xwb = (X[\"age\"] *self.w1) + (X[\"affordibility\"] *self.w1) + self.b \n",
    "        prediction = sigmoid(f_xwb)\n",
    "        list1 = [] \n",
    "        for i in prediction:\n",
    "            if i >= 0.50:\n",
    "                list1.append(1)\n",
    "            else:\n",
    "                list1.append(0)\n",
    "        return np.array(list1)\n",
    "                \n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.314815619925096, 1.4661987626399762, -3.0753522887044826)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myNN = NeuralNetwork()\n",
    "myNN.fit(X_train_scaled,y_train,400)\n",
    "myNN.w1,myNN.w2,myNN.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myNN.predict(X_test_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
